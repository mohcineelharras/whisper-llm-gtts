{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Source audio from local or internet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "pO-ezldtvjNe",
        "outputId": "ced964d7-8c0a-4d73-d945-d79c08434bd2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/home/mohcine/work/personal/whisper/output/output.mp3'"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from pytube import YouTube\n",
        "\n",
        "video_url = \"https://www.youtube.com/watch?v=gbjMZq56iws&t=81s\"\n",
        "YouTube(video_url).streams.filter(only_audio=True).first().download(filename=\"output/output.mp3\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "An AI, or artificial intelligence, refers to a computer system that can perform tasks that typically require human intelligence, such as visual perception, speech\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'\\nAn AI, or artificial intelligence, refers to a computer system that can perform tasks that typically require human intelligence, such as visual perception, speech'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain import OpenAI\n",
        "def basic_question(prompt):\n",
        "  llm = OpenAI(\n",
        "    openai_api_key = \"anyValueYouLike\",\n",
        "    temperature = 0,\n",
        "    openai_api_base = \"http://172.19.208.1:1300/v1\",\n",
        "    max_tokens = 30,\n",
        "  )\n",
        "  response = llm(prompt=prompt)\n",
        "  print(response)\n",
        "  return response\n",
        "\n",
        "basic_question(\"What is an AI?\")\n",
        "#basic_question(\"say this is a test\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Audio to text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### whisper v3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "X4idwYSAtYWC",
        "outputId": "ca98a84d-13da-4131-c040-ff3f59e44351"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(task=\"automatic-speech-recognition\",\n",
        "                model=\"openai/whisper-large-v3\",\n",
        "                torch_dtype=torch.\n",
        "                float16,\n",
        "                device=\"cuda:0\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cyvj_J4duVcy",
        "outputId": "e493016b-8e12-4c55-8eae-b5d80fe5e548"
      },
      "outputs": [],
      "source": [
        "result_text = pipe(\"output.mp3\")[\"text\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### distil-whisper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
        "\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
        "model_id = \"distil-whisper/distil-large-v2\"\n",
        "\n",
        "model = AutoModelForSpeechSeq2Seq.from_pretrained(model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True)\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(model_id)\n",
        "\n",
        "pipe = pipeline(\n",
        "    \"automatic-speech-recognition\",\n",
        "    model=model,\n",
        "    tokenizer=processor.tokenizer,\n",
        "    feature_extractor=processor.feature_extractor,\n",
        "    max_new_tokens=256,\n",
        "    chunk_length_s=15,\n",
        "    batch_size=16,\n",
        "    torch_dtype=torch_dtype,\n",
        "    device=device,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\" In a previous video, I showed you a number of demos where people were using DPT4WVision and the Open AI Text to Speech API to create video narrators. So I'll walk you through a step-by-step process of how exactly to do that inside a Google CoLab notebook. Everything you're about to see is based on this bookbook from OpenE Act. I'm going to put a link to this in the description. Okay, so here is how the workflow is going to look like. We will provide an input video. Now GPD4 does not understand videos by itself. We will have to convert those into frames and then feed those frames into GPT4 with vision. Then in GPT4 with vision will generate a description of all the frames so whatever it sees in the frames. Then we will use the newly released text-to-speech API from OpenAI to convert that description into speech. And then we will need to combine both the original input video as well as the generated audio to create a new video. Now I'll walk you through a step-by-step process how to do this exactly. Now keep in mind that this is not going to be a completely automated process yet, but I think GPD4 is going to have video understanding abilities pretty soon. I'm going to be doing everything inside a Google CoLab notebook and I'll share a link to this notebook in the description. First and foremost we need a video to process. For this example, we are going to be using this video of football, which is soccer for our American friends. So first, we need to upload that video to Google CoLab, and the way you do it is you click on this icon then right click and then click upload the file upload that file I have already uploaded a file I'm calling this video.MP4. Now let's look at the code. The first we are importing all the different packages that we will need. In this case we're using the open CV package for video processing. Next I'm setting up my open AI API key. I'm going to revoke this after the video. Now once we have an input video, we need to convert this into frames and for that I'm using this function. The code is based on the open AI cookbook but I converted this into a function so that it's reusable. So let me walk you through the code. First we are reading the video using open CV, then we are extracting the total number of frames present in the video. This will be needed in order to compute the duration of the video. After that, we are extracting individual frames from the video and putting them in a list. Once this list is created, we will release all the memory that is occupied by the video object. Then we are printing the number of frames that we get from the video and At the end we are returning the frames or list of frames as well as the total number of frames that we got now in order to use that function we first need to provide a path in this case I am providing the path of this video. MP4 file and you see that here. With this path, we call of function and it extracted a total of 826 frames from this video. We are also computing the duration based on the total number of frames and assuming the video has 30 frames per second. So the video is about 28 seconds long and this is pretty accurate based on the actual duration of the video. The next step in the process is to take those extracted video frames and generate a description using GPD4 with vision. Now for this type of the code that is provided in the cookbook on OpenAIS website, it actually does not work. The API call has changed. Let me show you how to correctly use this. Here's the code that converts sequence of images into text description. Now this is again based on the code that is provided in the cookbook. However, I made some changes so that it's compatible with the new API. So let me walk you through this. First you need to create an open AI client. So in that case you need to simply pass on the open AI API key. I'm doing this to the environment variable that I created. Now after that we need to set some parameters. The first one is prompt message. In this case the message is going to be coming from the user. That's why the rule is set to user. The contents is a user prompt that this function accepts. Based on the type of video that you upload you will want to modify this user prompt and I'll go I'll show you what that looks like now since we're using GPT forward vision so apart from the text prompt we can also provide images as well so in this case I am pre-processing the images. First of all I'm reducing the resolution of the image. High resolution is going to take more tokens, lower resolution is going to take less tokens and I want to save some tokens. So I'm converting the image or resizing the image to 428 pixels. Now the second part is I'm not feeding in every frame that was created but rather every 25th frame and the reason is that GPT4 simply needs to look at some middle frames in order to understand what is going on in the video. You don't have to feed in every frame. The other reason is that it will save you a lot of money because you're going to be making less API calls. Now for individual users, there is a limit on the number of tokens you can send what I had to do was I had to I think change this to every 60th frame but play around with it and see what kind of limitations in terms of number of tokens you run into. Now after that we need to set which model we want to use. We want to use GPD4 vision preview that's the GP4 turbo with vision capability and we pass on our messages so basically all the prompt so in that case it's the user role prompt that we are going to receive as well as the input images and I'm limiting the maximum number of tokens that is supposed to generate to 200 only. Again, this is because I ran into some issues when it came to the rate limit. So here is how you make the API call now with this new client and we print the results and return the results. Basically it will accept the list of frames that we created in the previous steps as an input along with the user prompt, which I'm about to show you. And as a result of this function call, you will get the description of all the frames. Okay, so here's the user prompt that I'm using. These are frames from a video of a soccer game. Create a commentary of what you see in the game. Then I'm providing the duration of the video in seconds and here is a very helpful trick that I've found, which is the transcription needs to be and you provide the number of words where you want to limit the transcription to. And the way I'm calculating is multiplying the duration by 2. So that means we're assuming there are two words per second. And this was a neat trick that I've found on AI Jason's video. I'll put the link to his channel. Really helpful. I would recommend everybody to watch it. Now another thing that I had to do was I provided the name of the player. DPT4 does not recognize people and then I said make the commentary exciting so here is what the transcription look like. Now a couple of things that I noticed when I was experimenting with it. If you don't provide the word count, the description might be cut off at the end. Because we are kind of limiting the maximum number of tokens. Now the second thing to keep in mind is even though if you provide exact work on end duration, the text that it generate may not convert into the exact duration that you want. So for example, in this case the video is about 28 seconds long but the description that it generates might be longer than that so you will probably need to do some post processing on top of the audio that it generates. Next we need to convert this text to speech. For that we are going to be using the newly released text to speech model from OpenEI. Now I wanted to highlight a few things before we use this model. So you are limited to only six different voice options. The second thing is that there is no direct mechanism to control emotional output of the audio generated. So you are completely reliant on open AI. There are no customization that you can make to the output audio. Okay, so here's the function which converts a transcription into audio. Here's how it works. First you need to make a post request the text to speech model and in that case you need to provide what a model you want to use so I think right now they only have this text to speech one as an option. Provide the transcription of your video. Then you can also choose voice, so it's one of the six options that you have, and the output is going to be an audio file. So we provide the transcript that we created in the previous step and we get an audio. Okay, so here is how it sounds like. Here he goes. Messie with the ball at his feet. A burst of speed past one defender. He's breaking through the midfield line like a hot knife through butter. He's approaching the penalty box, dodging another tackle, such agility. Even though the audio is great, but it lacks emotions, and unfortunately we cannot customize it today using OpenAI text-to-speech model. Now, another thing that I want to highlight is that the length of the audio that we got is around 28 seconds. In my case, it's exactly the same as the length of the original video. However, it might not be the case for you. I had to experiment with it quite a bit and that is the reason that I'm not actually going to write a script to put both the audio and video together into a new video but I'll rather do this manually. I'm going to manually download this file. Okay so in this last step we need to put everything together we'll take the original video and the audio that we generated and combine them together. Now for this step I'm personally using a software called Capcut. This is the software that I use for my video editing, but you can use whatever you like. So here I have the original video and the audio file as well, so I'm going to simply drag it in. Now as you can see it aligns pretty well in my case so you probably will have this mismatch but I think I'm pretty happy with this. So I'm going to just generate a video based on this. Okay, so this is one of the issues that you can run into. Sometimes you will have to increase the speed of the audio in order to correctly match it with the video. So I'm going to generate a video based on this. Here he goes, messy with the ball at his feet, a burst of speed past one defender, he's breaking through the midfield line like a hot knife through butter. He's approaching the penalty box, dodging another tackle, such agility. He's now one-on-one with the goalie, eyes locked, tension rises, messy faints, the keepers on the ground, a swift kick, and it's in, ghoul. Pure messy magic, ladies and gentlemen, an absolute spectacle of skill and precision. Now as you can see there was some misalignment. The video narration was I think lagging behind the actual video and I think this is happening because I had to skip a lot of frames in order to avoid that great limit from GP4 API but I feel like this approach and definitely has a lot of potential. People are building some really cool applications on top of it. In this video I wanted to build a very simple proof of concept. If you need any help with using GPT4 Vision API in your own projects, you can reach out to me. Details are going to be in the description of the video. I hope you found this video useful. Consider liking the video and subscribe to the channel. Thanks for watching and as always, you in the next one.\""
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result_text = pipe(\"output/output.mp3\")[\"text\"]\n",
        "with open('output/output.txt', 'w') as file:\n",
        "    # Your string to write to the file\n",
        "    my_string = result_text\n",
        "    \n",
        "    # Write the string to the file\n",
        "    file.write(my_string)\n",
        "\n",
        "result_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oCGbmcrGuVkk"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Text to speech"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### gtts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "High Performance MPEG 1.0/2.0/2.5 Audio Player for Layer 1, 2, and 3.\n",
            "Version 0.3.2-1 (2012/03/25). Written and copyrights by Joe Drew,\n",
            "now maintained by Nanakos Chrysostomos and others.\n",
            "Uses code from various people. See 'README' for more!\n",
            "THIS SOFTWARE COMES WITH ABSOLUTELY NO WARRANTY! USE AT YOUR OWN RISK!\n",
            "tcgetattr(): Inappropriate ioctl for device\n",
            "\n",
            "Playing MPEG stream from speech.mp3 ...\n",
            "MPEG 2.0 layer III, 32 kbit/s, 24000 Hz mono\n",
            "                                                                            \n",
            "[0:02] Decoding of speech.mp3 finished.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from gtts import gTTS\n",
        "import os\n",
        "\n",
        "language = \"fr\"\n",
        "#language = \"en\"\n",
        "\n",
        "text = \"Hello Mohcine, how are you today?\"\n",
        "tts = gTTS(text, lang='en', tld=\"us\")\n",
        "tts.save(\"speech.mp3\")\n",
        "\n",
        "# Play the converted file\n",
        "os.system(\"mpg321 speech.mp3\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### TTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " > tts_models/multilingual/multi-dataset/xtts_v2 is already downloaded.\n",
            " > Using model: xtts\n",
            " > Text splitted to sentences.\n",
            "[\"It took me quite a long time to develop a voice, and now that I have it I'm not going to be silent.\"]\n",
            " > Processing time: 18.099616050720215\n",
            " > Real-time factor: 3.0385593092823484\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'output/output.mp3'"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from TTS.api import TTS\n",
        "\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "tts = TTS(\"tts_models/multilingual/multi-dataset/xtts_v2\")\n",
        "tts.to(device=device)\n",
        "# generate speech by cloning a voice using default settings\n",
        "tts.tts_to_file(text=\"It took me quite a long time to develop a voice, and now that I have it I'm not going to be silent.\",\n",
        "                file_path=\"output/output.mp3\",\n",
        "                speaker_wav=\"output/output.mp3\",\n",
        "                language=\"en\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b22229f5094e43d3ab4c180b33bfd9ae",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "(…)suno/bark-small/resolve/main/config.json:   0%|          | 0.00/8.80k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fad09d070163487f8a1e3bca0e299d71",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/1.68G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1ea42593f36349258f70b637b326eda7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "(…)mall/resolve/main/generation_config.json:   0%|          | 0.00/4.91k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1e45fa533bac43e68c621cf8bb820350",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "(…)small/resolve/main/tokenizer_config.json:   0%|          | 0.00/353 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ab2621cc06ab44a897c5bcaae7faead4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "(…)o/suno/bark-small/resolve/main/vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "09a326f37197426dabdd1d63a61a9107",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "(…)o/bark-small/resolve/main/tokenizer.json:   0%|          | 0.00/2.92M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "db9ab0ea55ba4cfa8900fb770a623854",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "(…)all/resolve/main/special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:10000 for open-end generation.\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m/home/mohcine/work/personal/whisper/whisper.ipynb Cell 20\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/mohcine/work/personal/whisper/whisper.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mscipy\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/mohcine/work/personal/whisper/whisper.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m synthesiser \u001b[39m=\u001b[39m pipeline(\u001b[39m\"\u001b[39m\u001b[39mtext-to-speech\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39msuno/bark-small\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/mohcine/work/personal/whisper/whisper.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m speech \u001b[39m=\u001b[39m synthesiser(\u001b[39m\"\u001b[39;49m\u001b[39mHello, my dog is cooler than you!\u001b[39;49m\u001b[39m\"\u001b[39;49m, forward_params\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mdo_sample\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39mTrue\u001b[39;49;00m})\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/mohcine/work/personal/whisper/whisper.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m scipy\u001b[39m.\u001b[39mio\u001b[39m.\u001b[39mwavfile\u001b[39m.\u001b[39mwrite(\u001b[39m\"\u001b[39m\u001b[39mbark_out.wav\u001b[39m\u001b[39m\"\u001b[39m, rate\u001b[39m=\u001b[39mspeech[\u001b[39m\"\u001b[39m\u001b[39msampling_rate\u001b[39m\u001b[39m\"\u001b[39m], data\u001b[39m=\u001b[39mspeech[\u001b[39m\"\u001b[39m\u001b[39maudio\u001b[39m\u001b[39m\"\u001b[39m])\n",
            "File \u001b[0;32m~/mambaforge/envs/audio/lib/python3.11/site-packages/transformers/pipelines/text_to_audio.py:138\u001b[0m, in \u001b[0;36mTextToAudioPipeline.__call__\u001b[0;34m(self, text_inputs, **forward_params)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, text_inputs: Union[\u001b[39mstr\u001b[39m, List[\u001b[39mstr\u001b[39m]], \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mforward_params):\n\u001b[1;32m    123\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[39m    Generates speech/audio from the inputs. See the [`TextToAudioPipeline`] documentation for more information.\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[39m        - **sampling_rate** (`int`) -- The sampling rate of the generated audio waveform.\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 138\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(text_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mforward_params)\n",
            "File \u001b[0;32m~/mambaforge/envs/audio/lib/python3.11/site-packages/transformers/pipelines/base.py:1140\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1132\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39m(\n\u001b[1;32m   1133\u001b[0m         \u001b[39miter\u001b[39m(\n\u001b[1;32m   1134\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_iterator(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1137\u001b[0m         )\n\u001b[1;32m   1138\u001b[0m     )\n\u001b[1;32m   1139\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1140\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "File \u001b[0;32m~/mambaforge/envs/audio/lib/python3.11/site-packages/transformers/pipelines/base.py:1147\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1145\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_single\u001b[39m(\u001b[39mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[1;32m   1146\u001b[0m     model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreprocess(inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpreprocess_params)\n\u001b[0;32m-> 1147\u001b[0m     model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(model_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mforward_params)\n\u001b[1;32m   1148\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpostprocess(model_outputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpostprocess_params)\n\u001b[1;32m   1149\u001b[0m     \u001b[39mreturn\u001b[39;00m outputs\n",
            "File \u001b[0;32m~/mambaforge/envs/audio/lib/python3.11/site-packages/transformers/pipelines/base.py:1046\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1044\u001b[0m     \u001b[39mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1045\u001b[0m         model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m-> 1046\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward(model_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mforward_params)\n\u001b[1;32m   1047\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m   1048\u001b[0m \u001b[39melse\u001b[39;00m:\n",
            "File \u001b[0;32m~/mambaforge/envs/audio/lib/python3.11/site-packages/transformers/pipelines/text_to_audio.py:112\u001b[0m, in \u001b[0;36mTextToAudioPipeline._forward\u001b[0;34m(self, model_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    109\u001b[0m kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(kwargs, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    111\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mcan_generate():\n\u001b[0;32m--> 112\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mgenerate(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    113\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    114\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)[\u001b[39m0\u001b[39m]\n",
            "File \u001b[0;32m~/mambaforge/envs/audio/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/mambaforge/envs/audio/lib/python3.11/site-packages/transformers/models/bark/modeling_bark.py:1591\u001b[0m, in \u001b[0;36mBarkModel.generate\u001b[0;34m(self, input_ids, history_prompt, **kwargs)\u001b[0m\n\u001b[1;32m   1583\u001b[0m semantic_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msemantic\u001b[39m.\u001b[39mgenerate(\n\u001b[1;32m   1584\u001b[0m     input_ids,\n\u001b[1;32m   1585\u001b[0m     history_prompt\u001b[39m=\u001b[39mhistory_prompt,\n\u001b[1;32m   1586\u001b[0m     semantic_generation_config\u001b[39m=\u001b[39msemantic_generation_config,\n\u001b[1;32m   1587\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs_semantic,\n\u001b[1;32m   1588\u001b[0m )\n\u001b[1;32m   1590\u001b[0m \u001b[39m# 2. Generate from the coarse model\u001b[39;00m\n\u001b[0;32m-> 1591\u001b[0m coarse_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcoarse_acoustics\u001b[39m.\u001b[39;49mgenerate(\n\u001b[1;32m   1592\u001b[0m     semantic_output,\n\u001b[1;32m   1593\u001b[0m     history_prompt\u001b[39m=\u001b[39;49mhistory_prompt,\n\u001b[1;32m   1594\u001b[0m     semantic_generation_config\u001b[39m=\u001b[39;49msemantic_generation_config,\n\u001b[1;32m   1595\u001b[0m     coarse_generation_config\u001b[39m=\u001b[39;49mcoarse_generation_config,\n\u001b[1;32m   1596\u001b[0m     codebook_size\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mcodebook_size,\n\u001b[1;32m   1597\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs_coarse,\n\u001b[1;32m   1598\u001b[0m )\n\u001b[1;32m   1600\u001b[0m \u001b[39m# 3. \"generate\" from the fine model\u001b[39;00m\n\u001b[1;32m   1601\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfine_acoustics\u001b[39m.\u001b[39mgenerate(\n\u001b[1;32m   1602\u001b[0m     coarse_output,\n\u001b[1;32m   1603\u001b[0m     history_prompt\u001b[39m=\u001b[39mhistory_prompt,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1608\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs_fine,\n\u001b[1;32m   1609\u001b[0m )\n",
            "File \u001b[0;32m~/mambaforge/envs/audio/lib/python3.11/site-packages/transformers/models/bark/modeling_bark.py:1004\u001b[0m, in \u001b[0;36mBarkCoarseModel.generate\u001b[0;34m(self, semantic_output, semantic_generation_config, coarse_generation_config, codebook_size, history_prompt, **kwargs)\u001b[0m\n\u001b[1;32m    990\u001b[0m input_coarse \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mhstack(\n\u001b[1;32m    991\u001b[0m     [\n\u001b[1;32m    992\u001b[0m         input_coarse,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    995\u001b[0m     ]\n\u001b[1;32m    996\u001b[0m )\n\u001b[1;32m    998\u001b[0m alternatingLogitsProcessor \u001b[39m=\u001b[39m AlternatingCodebooksLogitsProcessor(\n\u001b[1;32m    999\u001b[0m     input_coarse\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m],\n\u001b[1;32m   1000\u001b[0m     semantic_generation_config\u001b[39m.\u001b[39msemantic_vocab_size,\n\u001b[1;32m   1001\u001b[0m     codebook_size,\n\u001b[1;32m   1002\u001b[0m )\n\u001b[0;32m-> 1004\u001b[0m output_coarse \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mgenerate(\n\u001b[1;32m   1005\u001b[0m     input_coarse,\n\u001b[1;32m   1006\u001b[0m     logits_processor\u001b[39m=\u001b[39;49m[alternatingLogitsProcessor],\n\u001b[1;32m   1007\u001b[0m     max_new_tokens\u001b[39m=\u001b[39;49m\u001b[39mmin\u001b[39;49m(sliding_window_len, max_generated_len \u001b[39m-\u001b[39;49m total_generated_len),\n\u001b[1;32m   1008\u001b[0m     generation_config\u001b[39m=\u001b[39;49mcoarse_generation_config,\n\u001b[1;32m   1009\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   1010\u001b[0m )\n\u001b[1;32m   1012\u001b[0m input_coarse_len \u001b[39m=\u001b[39m input_coarse\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n\u001b[1;32m   1014\u001b[0m x_coarse \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mhstack([x_coarse, output_coarse[:, input_coarse_len:]])\n",
            "File \u001b[0;32m~/mambaforge/envs/audio/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/mambaforge/envs/audio/lib/python3.11/site-packages/transformers/generation/utils.py:1648\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1640\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1641\u001b[0m         input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1642\u001b[0m         expand_size\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1643\u001b[0m         is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1644\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1645\u001b[0m     )\n\u001b[1;32m   1647\u001b[0m     \u001b[39m# 13. run sample\u001b[39;00m\n\u001b[0;32m-> 1648\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msample(\n\u001b[1;32m   1649\u001b[0m         input_ids,\n\u001b[1;32m   1650\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mlogits_processor,\n\u001b[1;32m   1651\u001b[0m         logits_warper\u001b[39m=\u001b[39;49mlogits_warper,\n\u001b[1;32m   1652\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mstopping_criteria,\n\u001b[1;32m   1653\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mpad_token_id,\n\u001b[1;32m   1654\u001b[0m         eos_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[1;32m   1655\u001b[0m         output_scores\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49moutput_scores,\n\u001b[1;32m   1656\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mreturn_dict_in_generate,\n\u001b[1;32m   1657\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   1658\u001b[0m         streamer\u001b[39m=\u001b[39;49mstreamer,\n\u001b[1;32m   1659\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   1660\u001b[0m     )\n\u001b[1;32m   1662\u001b[0m \u001b[39melif\u001b[39;00m generation_mode \u001b[39m==\u001b[39m GenerationMode\u001b[39m.\u001b[39mBEAM_SEARCH:\n\u001b[1;32m   1663\u001b[0m     \u001b[39m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   1664\u001b[0m     beam_scorer \u001b[39m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   1665\u001b[0m         batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[1;32m   1666\u001b[0m         num_beams\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1671\u001b[0m         max_length\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mmax_length,\n\u001b[1;32m   1672\u001b[0m     )\n",
            "File \u001b[0;32m~/mambaforge/envs/audio/lib/python3.11/site-packages/transformers/generation/utils.py:2730\u001b[0m, in \u001b[0;36mGenerationMixin.sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2727\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2729\u001b[0m \u001b[39m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2730\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\n\u001b[1;32m   2731\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs,\n\u001b[1;32m   2732\u001b[0m     return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   2733\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   2734\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   2735\u001b[0m )\n\u001b[1;32m   2737\u001b[0m \u001b[39mif\u001b[39;00m synced_gpus \u001b[39mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2738\u001b[0m     \u001b[39mcontinue\u001b[39;00m  \u001b[39m# don't waste resources running the code we don't need\u001b[39;00m\n",
            "File \u001b[0;32m~/mambaforge/envs/audio/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/mambaforge/envs/audio/lib/python3.11/site-packages/transformers/models/bark/modeling_bark.py:647\u001b[0m, in \u001b[0;36mBarkCausalModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, position_ids, head_mask, labels, input_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    639\u001b[0m     outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    640\u001b[0m         create_custom_forward(block),\n\u001b[1;32m    641\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    644\u001b[0m         head_mask[i],\n\u001b[1;32m    645\u001b[0m     )\n\u001b[1;32m    646\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 647\u001b[0m     outputs \u001b[39m=\u001b[39m block(\n\u001b[1;32m    648\u001b[0m         hidden_states,\n\u001b[1;32m    649\u001b[0m         past_key_values\u001b[39m=\u001b[39;49mpast_layer_key_values,\n\u001b[1;32m    650\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    651\u001b[0m         head_mask\u001b[39m=\u001b[39;49mhead_mask[i],\n\u001b[1;32m    652\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    653\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    654\u001b[0m     )\n\u001b[1;32m    656\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    658\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
            "File \u001b[0;32m~/mambaforge/envs/audio/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/mambaforge/envs/audio/lib/python3.11/site-packages/transformers/models/bark/modeling_bark.py:256\u001b[0m, in \u001b[0;36mBarkBlock.forward\u001b[0;34m(self, hidden_states, past_key_values, attention_mask, head_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    253\u001b[0m outputs \u001b[39m=\u001b[39m attn_outputs[\u001b[39m1\u001b[39m:]\n\u001b[1;32m    255\u001b[0m intermediary_hidden_states \u001b[39m=\u001b[39m hidden_states \u001b[39m+\u001b[39m attn_output\n\u001b[0;32m--> 256\u001b[0m intermediary_hidden_states \u001b[39m=\u001b[39m intermediary_hidden_states \u001b[39m+\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmlp(\n\u001b[1;32m    257\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayernorm_2(intermediary_hidden_states)\n\u001b[1;32m    258\u001b[0m )\n\u001b[1;32m    260\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n\u001b[1;32m    261\u001b[0m     outputs \u001b[39m=\u001b[39m (intermediary_hidden_states,) \u001b[39m+\u001b[39m outputs\n",
            "File \u001b[0;32m~/mambaforge/envs/audio/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/mambaforge/envs/audio/lib/python3.11/site-packages/transformers/models/bark/modeling_bark.py:207\u001b[0m, in \u001b[0;36mBarkMLP.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden_states):\n\u001b[0;32m--> 207\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49min_proj(hidden_states)\n\u001b[1;32m    208\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgelu(hidden_states)\n\u001b[1;32m    209\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mout_proj(hidden_states)\n",
            "File \u001b[0;32m~/mambaforge/envs/audio/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/mambaforge/envs/audio/lib/python3.11/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "import scipy\n",
        "\n",
        "synthesiser = pipeline(\"text-to-speech\", \"suno/bark-small\")\n",
        "\n",
        "speech = synthesiser(\"Hello, my dog is cooler than you!\", forward_params={\"do_sample\": True})\n",
        "\n",
        "scipy.io.wavfile.write(\"bark_out.wav\", rate=speech[\"sampling_rate\"], data=speech[\"audio\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Recording WAVE 'output/recording.wav' : Signed 16 bit Little Endian, Rate 44100 Hz, Stereo\n",
            "LAME 3.100 64bits (http://lame.sf.net)\n",
            "Using polyphase lowpass filter, transition band: 18671 Hz - 19205 Hz\n",
            "Encoding output/recording.wav to output/recording.mp3\n",
            "Encoding as 44.1 kHz j-stereo MPEG-1 Layer III (7.3x) 192 kbps qval=3\n",
            "    Frame          |  CPU time/estim | REAL time/estim | play/CPU |    ETA \n",
            "     0/       ( 0%)|    0:00/     :  |    0:00/     :  |         x|     :  \n",
            "00:10--------------------------------------------------------------------------\u001b[K\n",
            "   kbps      %     %\u001b[K\n",
            "     0/384    ( 0%)|    0:00/    0:00|    0:00/    0:00|   0.0000x|    0:00 \n",
            "00:10--------------------------------------------------------------------------\u001b[K\n",
            "   kbps      %     %\u001b[K\n",
            "   100/384    (26%)|    0:00/    0:00|    0:00/    0:00|   165.98x|    0:00 \n",
            "--------------------00:07------------------------------------------------------\u001b[K\n",
            "   kbps        MS  %     long  %\u001b[K\n",
            "   200/384    (52%)|    0:00/    0:00|    0:00/    0:00|   171.50x|    0:00 \n",
            "---------------------------------------00:04-----------------------------------\u001b[K\n",
            "   kbps        MS  %     long  %\u001b[K\n",
            "   300/384    (78%)|    0:00/    0:00|    0:00/    0:00|   174.23x|    0:00 \n",
            "----------------------------------------------------------00:02----------------\u001b[K\n",
            "   kbps        MS  %     long  %\u001b[K\n",
            "   384/384   (100%)|    0:00/    0:00|    0:00/    0:00|   174.96x|    0:00 \n",
            "-------------------------------------------------------------------------------\u001b[K\n",
            "   kbps        MS  %     long  %\u001b[K\n",
            "  192.0      100.0       100.0\u001b[K\n",
            "Writing LAME Tag...done\n",
            "ReplayGain: +64.8dB\n",
            "WARNING: ReplayGain exceeds the -51dB to +51dB range. Such a result is too\n",
            "         high to be stored in the header.\n"
          ]
        }
      ],
      "source": [
        "import subprocess\n",
        "import os\n",
        "\n",
        "def record_audio(duration=10, filename=\"output/recording\"):\n",
        "    wav_filename = f\"{filename}.wav\"\n",
        "    mp3_filename = f\"{filename}.mp3\"\n",
        "\n",
        "    # Record audio in WAV format\n",
        "    arecord_command = [\"arecord\", \"-d\", str(duration), \"-f\", \"cd\", \"-t\", \"wav\", wav_filename]\n",
        "    subprocess.run(arecord_command)\n",
        "\n",
        "    # Convert WAV to MP3\n",
        "    lame_command = [\"lame\", \"-b\", \"192\", wav_filename, mp3_filename]\n",
        "    subprocess.run(lame_command)\n",
        "\n",
        "    # Optionally, remove the WAV file after conversion\n",
        "    os.remove(wav_filename)\n",
        "\n",
        "    return mp3_filename\n",
        "\n",
        "mp3_file = record_audio()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Recording...\n",
            "Recording finished. File saved as output/recording.wav\n"
          ]
        }
      ],
      "source": [
        "import sounddevice as sd\n",
        "import soundfile as sf\n",
        "import numpy as np\n",
        "\n",
        "def record_audio(duration=3, fs=44100, filename=\"output/recording.wav\"):\n",
        "    \"\"\"\n",
        "    Record audio from the microphone and save it to a file.\n",
        "\n",
        "    Args:\n",
        "    duration (int): Duration of the recording in seconds.\n",
        "    fs (int): Sampling rate (samples per second).\n",
        "    filename (str): Path to save the audio file.\n",
        "    \"\"\"\n",
        "    print(\"Recording...\")\n",
        "    audio_data = sd.rec(int(duration * fs), samplerate=fs, channels=1)\n",
        "    sd.wait()  # Wait until recording is finished\n",
        "    sf.write(filename, audio_data, fs)\n",
        "    print(f\"Recording finished. File saved as {filename}\")\n",
        "\n",
        "# Example usage\n",
        "record_audio()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Recording...\n",
            "Recording finished, saving file...\n",
            "File saved as output/recording.wav\n"
          ]
        }
      ],
      "source": [
        "import sounddevice as sd\n",
        "import soundfile as sf\n",
        "import numpy as np\n",
        "\n",
        "def record_audio(duration=10, fs=44100, filename=\"output/recording.wav\"):\n",
        "    try:\n",
        "        print(\"Recording...\")\n",
        "        audio_data = sd.rec(int(duration * fs), samplerate=fs, channels=2)\n",
        "        sd.wait()  # Wait until recording is finished\n",
        "        print(\"Recording finished, saving file...\")\n",
        "        sf.write(filename, audio_data, fs)\n",
        "        print(f\"File saved as {filename}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "# Example usage\n",
        "record_audio()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d8385d8056924e57820d552646d68875",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5136b59ff6224a0eae3c8d0d50db52ac",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from ctransformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"TheBloke/dolphin-2.2.1-mistral-7B-GGUF\",\n",
        "                                            model_file=\"dolphin-2.2.1-mistral-7b.Q4_K_M.gguf\",\n",
        "                                            model_type=\"mistral\",\n",
        "                                            gpu_layers=20,\n",
        "                                            max_new_tokens=50,\n",
        "                                            mlock=True,\n",
        "                                            threads=8,\n",
        "                                            temperature=0.2)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Config(top_k=40, top_p=0.95, temperature=0.8, repetition_penalty=1.1, last_n_tokens=64, seed=-1, batch_size=8, threads=-1, max_new_tokens=50, stop=None, stream=False, reset=True, context_length=-1, gpu_layers=20, mmap=True, mlock=False)"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.config\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
